https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/?utm_content=cmp-true
https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html
########Spark SQL###########
https://spark.apache.org/docs/3.2.0/sql-programming-guide.html

###############Creating Data Frame##############
 #Create list of data to prepare data frame
    person_list = [("Berry","","Allen",1,"M"),
        ("Oliver","Queen","",2,"M"),
        ("Robert","","Williams",3,"M"),
        ("Tony","","Stark",4,"F"),
        ("Rajiv","Mary","Kumar",5,"F")
    ]
    

    #defining schema for dataset
    schema = StructType([ \
        StructField("firstname",StringType(),True), \
        StructField("middlename",StringType(),True), \
        StructField("lastname",StringType(),True), \
        StructField("id", IntegerType(), True), \
        StructField("gender", StringType(), True), \
      
    ])
    
    #creating spark dataframe
    df = spark.createDataFrame(data=person_list,schema=schema)

    #Printing data frame schema
    df.printSchema()

    #Printing data
    df.show(truncate=False)

    #Writing file in hadoop
    df.write.csv("record.csv")
##############################Check any directory already created or not in HDFS file#####################
In HDFS file system everything start from root-> Root means /
hadoof fs -ls /
########Naviage to the folder
hadoop fs -ls /user
hadoop fs -ls/user/abc
#### to make folder in HDFS#######
hadoop fs -mkdir /input_data
###To upload file in the HDFS newly created folder####
hadoop fs -put department.csv /input_data
hadoop fs -ls /input_data
https://spark.apache.org/docs/latest/sql-data-sources-csv.html
df1=spark.read.option("header",True).option("delimiter","|").option("").csv("/input_data/departments.csv")
#####If the HDFS file system availavle at somewhere else########
df1=spark.read.option("header",True).option("delimiter","|").option("").csv(" hdfs://name node:8888 end_point(ip addresss) /input_data/departments.csv")
>>> df1=spark.read.option("header",True).csv("/input_data/departments.csv")
>>> df1.printSchema()
root
 |-- DEPARTMENT_ID: string (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: string (nullable = true)
>>> df2=spark.read.option("header",True).option("inferSchema",True).csv("/input_data/departments.csv")
>>> df2.printSchema()
root
 |-- DEPARTMENT_ID: integer (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: integer (nullable = true)
#############Reference SQL sunction in Spark##################
https://sparkbyexamples.com/pyspark/pyspark-sql-functions/
>>> from pyspark.sql.functions import col
>>> empDf.select(col("EMPLOYEE_ID"),col("FIRST_NAME")).show()
+-----------+----------+
|EMPLOYEE_ID|FIRST_NAME|
+-----------+----------+
|        198|    Donald|
|        199|   Douglas|
|        200|  Jennifer|
|        201|   Michael|
|        202|       Pat|
|        203|     Susan|
|        204|   Hermann|
|        205|   Shelley|
|        206|   William|
|        100|    Steven|
|        101|     Neena|
|        102|       Lex|
|        103| Alexander|
|        104|     Bruce|
|        105|     David|
|        106|     Valli|
|        107|     Diana|
|        108|     Nancy|
|        109|    Daniel|
|        110|      John|
+-----------+----------+
only showing top 20 rows
>>> empDf.select(col("EMPLOYEE_ID").alias("E_ID"), col("FIRST_NAME").alias("F_NAME")).show()
+----+---------+
|E_ID|   F_NAME|
+----+---------+
| 198|   Donald|
| 199|  Douglas|
| 200| Jennifer|
| 201|  Michael|
| 202|      Pat|
| 203|    Susan|
| 204|  Hermann|
| 205|  Shelley|
| 206|  William|
| 100|   Steven|
| 101|    Neena|
| 102|      Lex|
| 103|Alexander|
| 104|    Bruce|
| 105|    David|
| 106|    Valli|
| 107|    Diana|
| 108|    Nancy|
| 109|   Daniel|
| 110|     John|
+----+---------+
only showing top 20 rows

>>> empDf.select("EMPLOYEE_ID","FIRST_NAME","SALARY").withColumn("NEW_SALARY",col("SALARY")+1000).show()
+-----------+----------+------+----------+
|EMPLOYEE_ID|FIRST_NAME|SALARY|NEW_SALARY|
+-----------+----------+------+----------+
|        198|    Donald|  2600|      3600|
|        199|   Douglas|  2600|      3600|
|        200|  Jennifer|  4400|      5400|
|        201|   Michael| 13000|     14000|
|        202|       Pat|  6000|      7000|
|        203|     Susan|  6500|      7500|
|        204|   Hermann| 10000|     11000|
|        205|   Shelley| 12008|     13008|
|        206|   William|  8300|      9300|
|        100|    Steven| 24000|     25000|
|        101|     Neena| 17000|     18000|
|        102|       Lex| 17000|     18000|
|        103| Alexander|  9000|     10000|
|        104|     Bruce|  6000|      7000|
|        105|     David|  4800|      5800|
|        106|     Valli|  4800|      5800|
|        107|     Diana|  4200|      5200|
|        108|     Nancy| 12008|     13008|
|        109|    Daniel|  9000|     10000|
|        110|      John|  8200|      9200|
+-----------+----------+------+----------+
only showing top 20 rows
>>> empDf.withColumn("NEW_SALARY",col("SALARY")+1000).select("EMPLOYEE_ID","FIRST_NAME","NEW_SALARY").show()
+-----------+----------+----------+
|EMPLOYEE_ID|FIRST_NAME|NEW_SALARY|
+-----------+----------+----------+
|        198|    Donald|      3600|
|        199|   Douglas|      3600|
|        200|  Jennifer|      5400|
|        201|   Michael|     14000|
|        202|       Pat|      7000|
|        203|     Susan|      7500|
|        204|   Hermann|     11000|
|        205|   Shelley|     13008|
|        206|   William|      9300|
|        100|    Steven|     25000|
|        101|     Neena|     18000|
|        102|       Lex|     18000|
|        103| Alexander|     10000|
|        104|     Bruce|      7000|
|        105|     David|      5800|
|        106|     Valli|      5800|
|        107|     Diana|      5200|
|        108|     Nancy|     13008|
|        109|    Daniel|     10000|
|        110|      John|      9200|
+-----------+----------+----------+
only showing top 20 rows

