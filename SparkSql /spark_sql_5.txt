>>> empDf=spark.read.option("header",True).option("inferSchema",True).csv("/input_data/employees.csv")
>>> empDf.show()                                                                
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+
|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+
|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|            - |       124|           50|
|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|            - |       124|           50|
|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|            - |       101|           10|
|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|
|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|
|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|
|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|
|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|
|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|
|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|
|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP| 17000|            - |       100|           90|
|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP| 17000|            - |       100|           90|
|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|  9000|            - |       102|           60|
|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|  6000|            - |       103|           60|
|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|   IT_PROG|  4800|            - |       103|           60|
|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|   IT_PROG|  4800|            - |       103|           60|
|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|   IT_PROG|  4200|            - |       103|           60|
|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR| 12008|            - |       101|          100|
|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|  9000|            - |       108|          100|
|        110|      John|     Chen|   JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|  8200|            - |       108|          100|
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+
only showing top 20 rows

>>> from pyspark.sql.functions import *
>>> empDf.count()
50
>>> empDf.select(count("salary")).show()
+-------------+
|count(salary)|
+-------------+
|           50|
+-------------+

>>> empDf.select(count("salary").alias("emp_salary")).show()
+----------+
|emp_salary|
+----------+
|        50|
+----------+

>>> empDf.select(max("salary").alias("Max_salary")).show()
+----------+
|Max_salary|
+----------+
|     24000|
+----------+

>>> empDf.select(min("salary").alias("Min_salary")).show()
+----------+
|Min_salary|
+----------+
|      2100|
+----------+

>>> empDf.select(sum("salary").alias("Sum_Salary")).show()
+----------+
|Sum_Salary|
+----------+
|    309116|
+----------+

>>> empDf.select("EMPLOYEE_ID", sum("salary").alias("sum_salary")).show()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/spark/python/pyspark/sql/dataframe.py", line 1685, in select
    jdf = self._jdf.select(self._jcols(*cols))
  File "/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/usr/local/spark/python/pyspark/sql/utils.py", line 117, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: grouping expressions sequence is empty, and 'EMPLOYEE_ID' is not an aggregate function. Wrap '(sum(salary) AS sum_salary)' in windowing function(s) or wrap 'EMPLOYEE_ID' in first() (or first_value) if you don't care which value you get.;
Aggregate [EMPLOYEE_ID#16, sum(salary#23) AS sum_salary#176L]
+- Relation [EMPLOYEE_ID#16,FIRST_NAME#17,LAST_NAME#18,EMAIL#19,PHONE_NUMBER#20,HIRE_DATE#21,JOB_ID#22,SALARY#23,COMMISSION_PCT#24,MANAGER_ID#25,DEPARTMENT_ID#26] csv

>>> empDf.select("EMPLOYEE_ID","FIRST_NAME","DEPARTMENT_ID").orderBy("salary").show()
+-----------+----------+-------------+
|EMPLOYEE_ID|FIRST_NAME|DEPARTMENT_ID|
+-----------+----------+-------------+
|        132|        TJ|           50|
|        136|     Hazel|           50|
|        128|    Steven|           50|
|        127|     James|           50|
|        135|        Ki|           50|
|        131|     James|           50|
|        119|     Karen|           30|
|        140|    Joshua|           50|
|        198|    Donald|           50|
|        199|   Douglas|           50|
|        118|       Guy|           30|
|        126|     Irene|           50|
|        139|      John|           50|
|        130|     Mozhe|           50|
|        117|     Sigal|           30|
|        116|    Shelli|           30|
|        134|   Michael|           50|
|        115| Alexander|           30|
|        125|     Julia|           50|
|        138|   Stephen|           50|
+-----------+----------+-------------+
only showing top 20 rows
>>> empDf.select("EMPLOYEE_ID","FIRST_NAME","DEPARTMENT_ID","salary").orderBy("salary").show()
+-----------+----------+-------------+------+
|EMPLOYEE_ID|FIRST_NAME|DEPARTMENT_ID|salary|
+-----------+----------+-------------+------+
|        132|        TJ|           50|  2100|
|        136|     Hazel|           50|  2200|
|        128|    Steven|           50|  2200|
|        127|     James|           50|  2400|
|        135|        Ki|           50|  2400|
|        131|     James|           50|  2500|
|        119|     Karen|           30|  2500|
|        140|    Joshua|           50|  2500|
|        198|    Donald|           50|  2600|
|        199|   Douglas|           50|  2600|
|        118|       Guy|           30|  2600|
|        126|     Irene|           50|  2700|
|        139|      John|           50|  2700|
|        130|     Mozhe|           50|  2800|
|        117|     Sigal|           30|  2800|
|        116|    Shelli|           30|  2900|
|        134|   Michael|           50|  2900|
|        115| Alexander|           30|  3100|
|        125|     Julia|           50|  3200|
|        138|   Stephen|           50|  3200|
+-----------+----------+-------------+------+
only showing top 20 rows
##############GroupBy############
>>> empDf.groupBy("DEPARTMENT_ID").sum("SALARY").show()
+-------------+-----------+
|DEPARTMENT_ID|sum(SALARY)|
+-------------+-----------+
|           20|      19000|
|           40|       6500|
|          100|      51608|
|           10|       4400|
|           50|      85600|
|           70|      10000|
|           90|      58000|
|           60|      28800|
|          110|      20308|
|           30|      24900|
+-------------+-----------+

>>> empDf.groupBy("DEPARTMENT_ID").max("SALARY").alias("Max_Salary").show()
+-------------+-----------+
|DEPARTMENT_ID|max(SALARY)|
+-------------+-----------+
|           20|      13000|
|           40|       6500|
|          100|      12008|
|           10|       4400|
|           50|       8200|
|           70|      10000|
|           90|      24000|
|           60|       9000|
|          110|      12008|
|           30|      11000|
+-------------+-----------+

>>> empDf.groupBy("DEPARTMENT_ID").min("SALARY").show()
+-------------+-----------+
|DEPARTMENT_ID|min(SALARY)|
+-------------+-----------+
|           20|       6000|
|           40|       6500|
|          100|       6900|
|           10|       4400|
|           50|       2100|
|           70|      10000|
|           90|      17000|
|           60|       4200|
|          110|       8300|
|           30|       2500|
+-------------+-----------+
>>> empDf.groupBy("DEPARTMENT_ID").max("SALARY").show()
+-------------+-----------+
|DEPARTMENT_ID|max(SALARY)|
+-------------+-----------+
|           20|      13000|
|           40|       6500|
|          100|      12008|
|           10|       4400|
|           50|       8200|
|           70|      10000|
|           90|      24000|
|           60|       9000|
|          110|      12008|
|           30|      11000|
+-------------+-----------+
###############GroupBy on 2 or more column##############
>>> empDf.groupBy("DEPARTMENT_ID","JOB_ID").sum("SALARY").show()
+-------------+----------+-----------+
|DEPARTMENT_ID|    JOB_ID|sum(SALARY)|
+-------------+----------+-----------+
|           90|   AD_PRES|      24000|
|           30|    PU_MAN|      11000|
|           70|    PR_REP|      10000|
|           50|    ST_MAN|      36400|
|           40|    HR_REP|       6500|
|           60|   IT_PROG|      28800|
|           10|   AD_ASST|       4400|
|           30|  PU_CLERK|      13900|
|           50|  ST_CLERK|      44000|
|           20|    MK_REP|       6000|
|           50|  SH_CLERK|       5200|
|           90|     AD_VP|      34000|
|          100|FI_ACCOUNT|      39600|
|          110|    AC_MGR|      12008|
|          110|AC_ACCOUNT|       8300|
|           20|    MK_MAN|      13000|
|          100|    FI_MGR|      12008|
+-------------+----------+-----------+
>>> empDf.groupBy("DEPARTMENT_ID","JOB_ID").sum("SALARY","EMPLOYEE_ID").show()
+-------------+----------+-----------+----------------+
|DEPARTMENT_ID|    JOB_ID|sum(SALARY)|sum(EMPLOYEE_ID)|
+-------------+----------+-----------+----------------+
|           90|   AD_PRES|      24000|             100|
|           30|    PU_MAN|      11000|             114|
|           70|    PR_REP|      10000|             204|
|           50|    ST_MAN|      36400|             610|
|           40|    HR_REP|       6500|             203|
|           60|   IT_PROG|      28800|             525|
|           10|   AD_ASST|       4400|             200|
|           30|  PU_CLERK|      13900|             585|
|           50|  ST_CLERK|      44000|            2120|
|           20|    MK_REP|       6000|             202|
|           50|  SH_CLERK|       5200|             397|
|           90|     AD_VP|      34000|             203|
|          100|FI_ACCOUNT|      39600|             555|
|          110|    AC_MGR|      12008|             205|
|          110|AC_ACCOUNT|       8300|             206|
|           20|    MK_MAN|      13000|             201|
|          100|    FI_MGR|      12008|             108|
+-------------+----------+-----------+----------------+
>>> empDf.groupBy("DEPARTMENT_ID").agg(sum("SALARY").alias("SUM_SALARY"), max("SALARY").alias("MAX_SALARY"), min("SALARY").alias("Min_Salary"),avg("SALARY").alias("Avg_Salary")).show()
+-------------+----------+----------+----------+------------------+
|DEPARTMENT_ID|SUM_SALARY|MAX_SALARY|Min_Salary|        Avg_Salary|
+-------------+----------+----------+----------+------------------+
|           20|     19000|     13000|      6000|            9500.0|
|           40|      6500|      6500|      6500|            6500.0|
|          100|     51608|     12008|      6900| 8601.333333333334|
|           10|      4400|      4400|      4400|            4400.0|
|           50|     85600|      8200|      2100|3721.7391304347825|
|           70|     10000|     10000|     10000|           10000.0|
|           90|     58000|     24000|     17000|19333.333333333332|
|           60|     28800|      9000|      4200|            5760.0|
|          110|     20308|     12008|      8300|           10154.0|
|           30|     24900|     11000|      2500|            4150.0|
+-------------+----------+----------+----------+------------------+
###With having clause#########
>>> empDf.groupBy("DEPARTMENT_ID").agg(sum("SALARY").alias("SUM_SALARY"), max("SALARY").alias("MAX_SALARY"), min("SALARY").alias("Min_Salary"),avg("SALARY").alias("Avg_Salary")).where(col("Max_SALARY")>=10000).show()
+-------------+----------+----------+----------+------------------+
|DEPARTMENT_ID|SUM_SALARY|MAX_SALARY|Min_Salary|        Avg_Salary|
+-------------+----------+----------+----------+------------------+
|           20|     19000|     13000|      6000|            9500.0|
|          100|     51608|     12008|      6900| 8601.333333333334|
|           70|     10000|     10000|     10000|           10000.0|
|           90|     58000|     24000|     17000|19333.333333333332|
|          110|     20308|     12008|      8300|           10154.0|
|           30|     24900|     11000|      2500|            4150.0|
+-------------+----------+----------+----------+------------------+
>>> df = empDf.withColumn("EMP_GRADE", when( col("SALARY") > 15000 , "A").when( (col("SALARY") >= 10000) & ( col("SALARY") < 15000), "B").otherwise("C"))
>>> df.select("EMPLOYEE_ID","SALARY","EMP_GRADE").show()
+-----------+------+---------+
|EMPLOYEE_ID|SALARY|EMP_GRADE|
+-----------+------+---------+
|        198|  2600|        C|
|        199|  2600|        C|
|        200|  4400|        C|
|        201| 13000|        B|
|        202|  6000|        C|
|        203|  6500|        C|
|        204| 10000|        B|
|        205| 12008|        B|
|        206|  8300|        C|
|        100| 24000|        A|
|        101| 17000|        A|
|        102| 17000|        A|
|        103|  9000|        C|
|        104|  6000|        C|
|        105|  4800|        C|
|        106|  4800|        C|
|        107|  4200|        C|
|        108| 12008|        B|
|        109|  9000|        C|
|        110|  8200|        C|
+-----------+------+---------+
only showing top 20 rows
>>> empDf.createOrReplaceTempView("employee")
>>> spark.sql("select * from employee limit 5").show()
+-----------+----------+---------+--------+------------+---------+--------+------+--------------+----------+-------------+
|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|  JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|
+-----------+----------+---------+--------+------------+---------+--------+------+--------------+----------+-------------+
|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|SH_CLERK|  2600|            - |       124|           50|
|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|SH_CLERK|  2600|            - |       124|           50|
|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03| AD_ASST|  4400|            - |       101|           10|
|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|  MK_MAN| 13000|            - |       100|           20|
|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|  MK_REP|  6000|            - |       201|           20|
+-----------+----------+---------+--------+------------+---------+--------+------+--------------+----------+-------------+
>>> df=spark.sql("select employee_id,salary from employee")
>>> df.show(100)
+-----------+------+
|employee_id|salary|
+-----------+------+
|        198|  2600|
|        199|  2600|
|        200|  4400|
|        201| 13000|
|        202|  6000|
|        203|  6500|
|        204| 10000|
|        205| 12008|
|        206|  8300|
|        100| 24000|
|        101| 17000|
|        102| 17000|
|        103|  9000|
|        104|  6000|
|        105|  4800|
|        106|  4800|
|        107|  4200|
|        108| 12008|
|        109|  9000|
|        110|  8200|
|        111|  7700|
|        112|  7800|
|        113|  6900|
|        114| 11000|
|        115|  3100|
|        116|  2900|
|        117|  2800|
|        118|  2600|
|        119|  2500|
|        120|  8000|
|        121|  8200|
|        122|  7900|
|        123|  6500|
|        124|  5800|
|        125|  3200|
|        126|  2700|
|        127|  2400|
|        128|  2200|
|        129|  3300|
|        130|  2800|
|        131|  2500|
|        132|  2100|
|        133|  3300|
|        134|  2900|
|        135|  2400|
|        136|  2200|
|        137|  3600|
|        138|  3200|
|        139|  2700|
|        140|  2500|
+-----------+------+
>>> spark.sql("select department_id, sum(salary) as sum_salary from employee group by department_id").show()
+-------------+----------+
|department_id|sum_salary|
+-------------+----------+
|           20|     19000|
|           40|      6500|
|          100|     51608|
|           10|      4400|
|           50|     85600|
|           70|     10000|
|           90|     58000|
|           60|     28800|
|          110|     20308|
|           30|     24900|
+-------------+----------+
